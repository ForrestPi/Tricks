{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from os import listdir, rename, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "from shutil import copyfile\n",
    "\n",
    "from keras.applications import densenet, xception\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dropout, Dense, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import utils\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from demo_utils import plot_history, plot_lr\n",
    "from model_utils import evaluate_model, get_checkpoint\n",
    "from clr import CyclicLR\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting allows Tensorflow to allocate GPU memory in runtime rather than at the session initialization. Remove this cell if you don't have GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'train'\n",
    "val_data_dir = 'validation'\n",
    "test_data_dir = 'test'\n",
    "\n",
    "img_height = 299\n",
    "img_width = 299\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "nb_train_samples, nb_validation_samples, nb_test_samples = (37184 + 25600, 12800, 25600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pseudolabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_tta</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2593_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758_1</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426_1</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted  predicted_tta  actual\n",
       "id                                      \n",
       "2593_1          1              1       1\n",
       "3758_1         38             38       1\n",
       "426_1          85             85       1\n",
       "4465_1          1              1       1\n",
       "6315_1          1              1       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('test_predictions.csv', index_col=0)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25600 images belonging to 128 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = image.ImageDataGenerator(preprocessing_function=xception.preprocess_input)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_class_dictionary = {validation_generator.class_indices[k]:k for k in validation_generator.class_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(validation_generator.filenames):\n",
    "    id = file.split('.')[0].split('/')[1]\n",
    "\n",
    "    label = labels.loc[id, 'predicted_tta']\n",
    "    label = inverse_class_dictionary[label - 1] # We have 1-based labels here, so we subtract 1.\n",
    "\n",
    "    source_file = join(test_data_dir, file)\n",
    "\n",
    "    target_name = f'{file.split(\".\")[0]}_psl.jpg'\n",
    "    target_file = join(train_data_dir, target_name)\n",
    "\n",
    "    copyfile(source_file, target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all needed functions and train model with pseudolabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from albumentations import Compose, OneOf, HorizontalFlip, RandomBrightness, RandomContrast, ShiftScaleRotate, HueSaturationValue\n",
    "from albumentations import MotionBlur, MedianBlur, Blur\n",
    "\n",
    "def preprocess_input_hard(image):    \n",
    "    augmentation = Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.15, scale_limit=0.3, rotate_limit=15, border_mode=cv2.BORDER_REPLICATE, p=0.3),\n",
    "        OneOf([\n",
    "            RandomBrightness(p=0.33, limit=0.15),\n",
    "            RandomContrast(p=0.33, limit=0.15),\n",
    "            HueSaturationValue(hue_shift_limit=0.15, sat_shift_limit=0.15, val_shift_limit=0.15, p=0.33)\n",
    "        ], p=0.3),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.33, blur_limit=4),\n",
    "            MedianBlur(p=0.33, blur_limit=4),\n",
    "            Blur(p=0.33, blur_limit=4)\n",
    "        ], p=0.25),\n",
    "    ], p=1.0)\n",
    "    \n",
    "    image_data = {'image': np.uint8(image)}\n",
    "    aug_image = augmentation(**image_data)['image']\n",
    "    \n",
    "    return xception.preprocess_input(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights (actual weights are different now because we changed class distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62784 images belonging to 128 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "train_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input_hard)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "# Class weights\n",
    "train_labels = utils.to_categorical(train_generator.classes)\n",
    "y_integers = np.argmax(train_labels, axis=1)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62784 images belonging to 128 classes.\n",
      "Found 12800 images belonging to 128 classes.\n",
      "Epoch 1/100\n",
      "3924/3924 [==============================] - 1970s 502ms/step - loss: 0.4646 - acc: 0.8749 - val_loss: 0.7754 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79039, saving model to weights-xception-6.hdf5\n",
      "Epoch 2/100\n",
      "3924/3924 [==============================] - 1975s 503ms/step - loss: 0.4582 - acc: 0.8767 - val_loss: 0.7662 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.79039 to 0.79148, saving model to weights-xception-6.hdf5\n",
      "Epoch 3/100\n",
      "3924/3924 [==============================] - 1975s 503ms/step - loss: 0.4530 - acc: 0.8777 - val_loss: 0.7551 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79148\n",
      "Epoch 4/100\n",
      "3924/3924 [==============================] - 1981s 505ms/step - loss: 0.4512 - acc: 0.8787 - val_loss: 0.7519 - val_acc: 0.7919\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79148 to 0.79187, saving model to weights-xception-6.hdf5\n",
      "Epoch 5/100\n",
      "3924/3924 [==============================] - 1980s 504ms/step - loss: 0.4511 - acc: 0.8773 - val_loss: 0.7502 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79187 to 0.79305, saving model to weights-xception-6.hdf5\n",
      "Epoch 6/100\n",
      "3924/3924 [==============================] - 1982s 505ms/step - loss: 0.4472 - acc: 0.8782 - val_loss: 0.7460 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79305\n",
      "Epoch 7/100\n",
      "3924/3924 [==============================] - 1985s 506ms/step - loss: 0.4419 - acc: 0.8798 - val_loss: 0.7421 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79305\n",
      "Epoch 8/100\n",
      "3924/3924 [==============================] - 1983s 505ms/step - loss: 0.4371 - acc: 0.8807 - val_loss: 0.7414 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79305\n",
      "Epoch 9/100\n",
      "3924/3924 [==============================] - 1985s 506ms/step - loss: 0.4418 - acc: 0.8804 - val_loss: 0.7405 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79305\n",
      "Epoch 10/100\n",
      "3924/3924 [==============================] - 1983s 505ms/step - loss: 0.4411 - acc: 0.8793 - val_loss: 0.7388 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79305\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "start_lr = 1e-5\n",
    "\n",
    "# Final learning rate is decreased since we do not train network from the beginning.\n",
    "end_lr = 1e-4\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "steps_per_epoch = nb_train_samples // batch_size\n",
    "\n",
    "base_model = xception.Xception(weights='imagenet',\n",
    "                            include_top=False,\n",
    "                            input_shape=(img_width, img_height, 3),\n",
    "                            pooling='avg')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "top_model = Sequential()    \n",
    "top_model.add(Dropout(0.4, name='top_dropout', input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(128, activation='softmax', name='top_softmax'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "model.load_weights(get_checkpoint('weights-xception-5'))\n",
    "\n",
    "model.compile(optimizer=SGD(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input_hard)\n",
    "test_datagen = image.ImageDataGenerator(preprocessing_function=xception.preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "clr = CyclicLR(base_lr=start_lr,\n",
    "               max_lr=end_lr,\n",
    "               step_size=2*steps_per_epoch,\n",
    "               scale_mode='triangular2')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=get_checkpoint('weights-xception-6'), verbose=1, monitor='val_acc', save_best_only=True)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1, monitor='val_acc', mode='max')\n",
    "\n",
    "history_6 = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=nb_train_samples // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            class_weight=class_weights_dict,\n",
    "                            callbacks=[checkpointer, early_stopping, clr],\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(get_checkpoint('weights-xception-6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_soft(image):    \n",
    "    augmentation = Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.10, scale_limit=0.10, rotate_limit=10, border_mode=cv2.BORDER_REPLICATE, p=0.25),\n",
    "        OneOf([\n",
    "            RandomBrightness(p=0.33, limit=0.1),\n",
    "            RandomContrast(p=0.33, limit=0.1), \n",
    "            HueSaturationValue(hue_shift_limit=0.1, sat_shift_limit=0.1, val_shift_limit=0.1, p=0.33),\n",
    "        ], p=0.25),\n",
    "    ], p=1.0)\n",
    "    \n",
    "    image_data = {'image': np.uint8(image)}\n",
    "    aug_image = augmentation(**image_data)['image']\n",
    "    \n",
    "    return xception.preprocess_input(aug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12800 images belonging to 128 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_tta</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2308</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4122</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89810</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89839</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  predicted  predicted_tta  actual\n",
       "0   2308          1              1       1\n",
       "1   3990          1              1       1\n",
       "2   4122        114              1       1\n",
       "3  89810        125              1       1\n",
       "4  89839          1              1       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = evaluate_model(model, 9, val_data_dir, preprocess_input_soft)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "results = pd.DataFrame(columns=['id', 'predicted', 'predicted_tta', 'actual'])\n",
    "\n",
    "predictions = np.argmax(probs[0], axis=1) + 1\n",
    "predictions_tta = np.argmax(np.mean(probs, axis=0), axis=1) + 1\n",
    "\n",
    "for i, file in enumerate(validation_generator.filenames):\n",
    "    id = file.split('_')[0].split('/')[1]\n",
    "    predicted = predictions[i]\n",
    "    predicted_tta = predictions_tta[i]\n",
    "    results.loc[i] = [id, predicted, predicted_tta, validation_generator.classes[i] + 1]\n",
    "    \n",
    "results['actual'] = results['actual'].astype(np.int32)\n",
    "results['predicted'] = results['predicted'].astype(np.int32)\n",
    "results['predicted_tta'] = results['predicted_tta'].astype(np.int32)\n",
    "    \n",
    "accuracy = accuracy_score(results['actual'], results['predicted'])\n",
    "accuracy_tta = accuracy_score(results['actual'], results['predicted_tta'])\n",
    "\n",
    "print(f'Accuracy: {accuracy}. Accuracy with TTA: {accuracy_tta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old accuracy with TTA: 0.7928125\n",
      "Accuracy: 0.793046875. Accuracy with TTA: 0.79734375\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(results['actual'], results['predicted'])\n",
    "accuracy_tta = accuracy_score(results['actual'], results['predicted_tta'])\n",
    "\n",
    "print('Old accuracy with TTA: 0.7928125')\n",
    "print(f'Accuracy: {accuracy}. Accuracy with TTA: {accuracy_tta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25600 images belonging to 128 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_tta</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2593</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3758</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>426</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4465</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  predicted  predicted_tta  actual\n",
       "0  2593          1              1       1\n",
       "1  3758         38             38       1\n",
       "2   426         85             85       1\n",
       "3  4465          1              1       1\n",
       "4  6315          1              1       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = evaluate_model(model, 9, test_data_dir, preprocess_input_soft)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "results = pd.DataFrame(columns=['id', 'predicted', 'predicted_tta', 'actual'])\n",
    "\n",
    "predictions = np.argmax(probs[0], axis=1) + 1\n",
    "predictions_tta = np.argmax(np.mean(probs, axis=0), axis=1) + 1\n",
    "\n",
    "for i, file in enumerate(validation_generator.filenames):\n",
    "    id = file.split('_')[0].split('/')[1]\n",
    "    predicted = predictions[i]\n",
    "    predicted_tta = predictions_tta[i]\n",
    "    results.loc[i] = [id, predicted, predicted_tta, validation_generator.classes[i] + 1]\n",
    "    \n",
    "results['actual'] = results['actual'].astype(np.int32)\n",
    "results['predicted'] = results['predicted'].astype(np.int32)\n",
    "results['predicted_tta'] = results['predicted_tta'].astype(np.int32)\n",
    "    \n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old accuracy with TTA: 0.80015625\n",
      "Accuracy: 0.803125. Accuracy with TTA: 0.80703125\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(results['actual'], results['predicted'])\n",
    "accuracy_tta = accuracy_score(results['actual'], results['predicted_tta'])\n",
    "\n",
    "print('Old accuracy with TTA: 0.7928125')\n",
    "print(f'Accuracy: {accuracy}. Accuracy with TTA: {accuracy_tta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
